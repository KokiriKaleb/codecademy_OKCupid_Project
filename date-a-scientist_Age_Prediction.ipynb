{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKCupid User Age Prediction Project\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "This project analyzes data from the online dating application OKCupid. In recent years, there has been a massive rise in the usage of dating apps to find love. Many of these apps use sophisticated data science techniques to recommend possible matches to users and to optimize the user experience. These apps give us access to a wealth of information that we've never had before about how different people experience romance. The goal of this project is to scope, prep, analyze, and create a machine learning model to solve a key question about the user base.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scoping**\n",
    "\n",
    "It's beneficial to create a project scope whenever a new project is being started. Below are four sections to help guide the project process and progress. The first section is the project goals, a section to define the high-level objectives and set the intentions for this project. The next section is the data; luckily, in this project, data is already provided but still needs to be checked if project goals can be met with the available data. Thirdly, the analysis will have to be thought through, which includes the methods and aligning the question(s) with the project goals. Lastly, evaluation will help build conclusions and findings from the analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project Goals**\n",
    "\n",
    "In this project, the goal is to utilize the skills learned through Codecademy and apply machine learning techniques to a data set. The primary research question that will be answered is **whether an OkCupid user's age can be accurately predicted using other variables from their profile**, with a special focus on their essay responses, education level, and lifestyle choices.\n",
    "\n",
    "This project is important as it provides value in multiple ways. For OkCupid, it could help identify profiles where age may be misrepresented, improving user trust and safety. Furthermore, it provides deep insight into the correlations between age and how users choose to describe themselves and their life stages, which can be used to further refine the user experience. This project will involve building a **regression model**, demonstrating a versatile and highly in-demand data science skill.\n",
    "\n",
    "---\n",
    "\n",
    "### **Data**\n",
    "\n",
    "The project has one data set provided by Codecademy called `profiles.csv`. In the data, each row represents an OkCupid user and the columns are the responses to their user profiles which include multi-choice and short answer questions. The target variable for this project will be the `age` column, a numerical value. Features will be drawn from the other columns, including categorical data like `education` and `drinks`, and unstructured text data from the `essay` columns.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analysis**\n",
    "\n",
    "This solution will use descriptive statistics and data visualization to find key figures in understanding the distribution of user ages and the relationships between age and other variables. Since the goal of the project is to predict a continuous numerical value (`age`), **regression algorithms** from the supervised learning family of machine learning models will be implemented.\n",
    "\n",
    "A significant part of the analysis will involve **feature engineering**, particularly processing the natural language in the `essay` columns to create meaningful numerical features. Techniques like TF-IDF or word counts may be used to quantify writing styles. The performance of various regression models, such as Linear Regression, Random Forest Regressor, and Gradient Boosting, will be compared to find the best-performing solution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation**\n",
    "\n",
    "The project will conclude with the evaluation of the final machine learning model on a validation data set. Because this is a regression task, the model's performance will be measured using metrics appropriate for continuous data. The output of the predictions will be checked using metrics such as:\n",
    "* **Mean Absolute Error (MAE):** Measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "* **Root Mean Squared Error (RMSE):** The standard deviation of the prediction errors, giving a sense of how concentrated the data is around the line of best fit. It penalizes larger errors more than MAE.\n",
    "* **R-squared ($R^2$):** The coefficient of determination, which represents the proportion of the variance in the `age` variable that is predictable from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "To begin the analysis, the profiles.csv dataset is loaded into a pandas DataFrame named profiles. The .head() method is then used to preview the first few rows, providing an initial look at the data's structure and confirming a successful import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.read_csv('profiles.csv', encoding='utf-8')\n",
    "profiles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.last_online.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Characteristics\n",
    "\n",
    "The `profiles` DataFrame contains 59,946 rows and 31 columns, providing a substantial dataset for this analysis. This volume of data is considered more than sufficient for machine learning, as it far exceeds the common rule of thumb of having at least 10 times as many data points as features (59,946 >> 10 * 31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Dictionary: `profiles.csv`\n",
    "\n",
    "| Column Name | Original Type | Refined Data Type | Rationale |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Demographics** | | | |\n",
    "| `sex` | Categorical | **Categorical (Binary)** | This is a categorical variable with only two options (m/f). |\n",
    "| `age` | Numerical | **Numerical (Integer)** | Age is a whole number. Matches the `int64` dtype. |\n",
    "| `orientation` | Categorical | **Categorical (Nominal)** | The categories have no intrinsic order. |\n",
    "| `ethnicity` | Categorical | **Categorical (Multi-value)** | A user can list multiple ethnicities, often stored as a comma-separated string. |\n",
    "| `location` | Categorical | **Geospatial/String** | More specific than generic 'Categorical'. Stored as a string (`object`). |\n",
    "| **Physical & Status** | | | |\n",
    "| `body_type` | Categorical | **Categorical (Nominal)** | Body type categories typically do not have a mathematical order. |\n",
    "| `height` | Numerical | **Numerical (Float)** | Height can include decimal values. Matches the `float64` dtype. |\n",
    "| `income` | Numerical | **Numerical (Integer)** | Annual income is represented as a whole number. Matches the `int64` dtype. |\n",
    "| `status` | Categorical | **Categorical (Nominal)** | Relationship statuses (e.g., single, married) have no inherent order. |\n",
    "| **Education & Work** | | | |\n",
    "| `education` | Categorical | **Categorical (Ordinal)** | There is a clear order in educational attainment (e.g., high school < bachelor's). |\n",
    "| `job` | Categorical | **Categorical (Nominal)** | Job categories generally have no intrinsic ranking. |\n",
    "| **Lifestyle & Family** | | | |\n",
    "| `diet` | Categorical | **Categorical (Nominal)** | Dietary habits are distinct categories without a clear mathematical order. |\n",
    "| `drinks` | Categorical | **Categorical (Ordinal)** | Alcohol consumption is often ranked (e.g., not at all < socially < often). |\n",
    "| `smokes` | Categorical | **Categorical (Ordinal)** | Smoking habits have a natural order (e.g., no < sometimes < yes). |\n",
    "| `drugs` | Categorical | **Categorical (Ordinal)** | Drug use habits also have a ranked order (e.g., never < sometimes). |\n",
    "| `pets` | Categorical | **Categorical (Multi-value)** | A user can have preferences for multiple types of pets. |\n",
    "| `offspring` | Categorical | **Categorical (Ordinal)** | There can be an order (e.g., doesn't have < has kids). |\n",
    "| `religion` | Categorical | **Categorical (Nominal)** | Religious beliefs are nominal, though the 'intensity' part could be ordinal. |\n",
    "| `sign` | Categorical | **Categorical (Nominal)** | Astrological signs have no order, though the 'intensity' part could be ordinal. |\n",
    "| **Communication & Essays** | | | |\n",
    "| `speaks` | Categorical | **Categorical (Multi-value)** | A user can speak multiple languages. |\n",
    "| `last_online` | Datetime | **Datetime** | The `info()` shows this as `object`, but it represents a specific point in time and should be converted. |\n",
    "| `essay0`-`essay9` | Text | **Text (String)** | Correctly identified as open-ended text. The `object` dtype in pandas is appropriate here. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A complete list of all categorical columns to be imputed\n",
    "all_categorical_cols = [\n",
    "    'offspring', 'religion', 'pets', 'drugs', 'smokes', 'diet', 'body_type', 'job',\n",
    "    'drinks', 'education', 'ethnicity', 'sign', 'speaks'\n",
    "]\n",
    "\n",
    "# List of all essay columns\n",
    "all_essay_cols = [f\"essay{i}\" for i in range(10)]\n",
    "\n",
    "# Impute all categorical columns\n",
    "for col in all_categorical_cols:\n",
    "    profiles[col].fillna('not specified', inplace=True)\n",
    "\n",
    "# Impute all essay columns\n",
    "for col in all_essay_cols:\n",
    "    profiles[col].fillna('', inplace=True)\n",
    "\n",
    "# Impute height with the median\n",
    "profiles['height'].fillna(profiles['height'].median(), inplace=True)\n",
    "\n",
    "# This will also return True if there are no null values.\n",
    "not profiles.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Target Variable `age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the 'age' column\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=profiles, x='age', bins=40, kde=True)\n",
    "plt.title('Distribution of User Ages')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.savefig('Distribution_of_User_Ages.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Peak Age Group**: The vast majority of users are in their mid-to-late 20s and early 30s. The single highest concentration of users appears to be around age 26.\n",
    "\n",
    "**Right-Skewed Distribution**: The data is heavily right-skewed, meaning there is a long tail of users extending into older age groups. After peaking in the late 20s, the number of users per age group steadily declines.\n",
    "\n",
    "**Potential Outliers**: There is a very long tail with users listed at improbable ages for a dating app, such as those over 100. These are almost certainly outliers or users entering joke data.\n",
    "\n",
    "**Outlier Handling is Necessary**: Before training our model we will filter out the extreme age outliers (e.g., anyone over a reasonable age like 80). These data points will negatively impact our model's performance and evaluation metrics.\n",
    "\n",
    "**Model Performance May Vary by Age**: The model will have a large amount of data to learn from for users in their 20s and 30s, so it will likely be most accurate in that range. It may be less accurate when predicting the age of older users simply because there is less data available for it to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Outlier Handling for Age ---\n",
    "\n",
    "# First, let's see how many rows we have before filtering\n",
    "print(f\"Original number of profiles: {len(profiles)}\")\n",
    "\n",
    "# Ages above this are rare and may be invalid data\n",
    "AGE_UPPER_LIMIT = 70\n",
    "\n",
    "# Create a new DataFrame that only includes users within the age limit\n",
    "profiles_filtered = profiles[profiles['age'] <= AGE_UPPER_LIMIT].copy()\n",
    "\n",
    "# Report how many outliers were removed\n",
    "print(f\"Number of profiles after removing users older than {AGE_UPPER_LIMIT}: {len(profiles_filtered)}\")\n",
    "print(f\"Number of outliers removed: {len(profiles) - len(profiles_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot to see age distribution by education level\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=profiles_filtered, x='education', y='age')\n",
    "plt.title('Age Distribution by Education Level')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Age')\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.savefig('Age_Distribution_by_Education_Level.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "* **Clear Age Progression:** There is a distinct upward trend in age as the level of education increases. Users with a high school education are generally the youngest, followed by those in college, then masters programs, and finally Ph.D., law, and med school graduates, who are the oldest.\n",
    "\n",
    "* **\"Working On\" vs. \"Graduated\":** For every level of schooling (college, masters, etc.), the \"working on\" category consistently shows a lower median age than the \"graduated from\" category. This is a powerful and logical signal.\n",
    "\n",
    "* **Distinct Groupings:** The plot clearly separates users into life-stage groups. For example, the median age for those \"working on college/university\" is in the low 20s, while the median for those who \"graduated from law school\" is in the mid-30s.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numerical columns for the correlation matrix\n",
    "numerical_cols = profiles_filtered.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numerical_cols.corr()\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.savefig('Correlation_Matrix_of_Numerical_Features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations \n",
    "\n",
    "* **No Correlation with `height` and `income`:** The correlation between `age` and `height` (-0.02) and `age` and `income` (-0.00) is effectively zero. This indicates there is no linear trend between these features. The lack of correlation with income is particularly interesting and may be due to how users reported their income.\n",
    "\n",
    "* **Weakest Signal from `essay_length`:** The correlation between `age` and `essay_length` is 0.08. While this is the highest correlation with `age` on the chart, it is still very weak. It suggests only a slight tendency for older users to write more, but the relationship is not strong.\n",
    "\n",
    "\n",
    "\n",
    "These simple numerical features, on their own, will not be strong predictors of `age`.\n",
    "\n",
    "This reinforces the importance of using **categorical features** (like `education`) and developing more advanced **NLP features** from the essay text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all essay columns into a single text block using the filtered DataFrame\n",
    "essay_cols = [f'essay{i}' for i in range(10)]\n",
    "profiles_filtered['all_essays'] = profiles_filtered[essay_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Calculate the length of the combined essays\n",
    "profiles_filtered['essay_length'] = profiles_filtered['all_essays'].str.len()\n",
    "\n",
    "# Create a scatter plot of essay length vs. age from the filtered data\n",
    "# Use a sample to avoid overplotting\n",
    "sample_df = profiles_filtered.sample(n=5000, random_state=42)\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.scatterplot(data=sample_df, x='age', y='essay_length', alpha=0.5)\n",
    "plt.title('Essay Length vs. Age (Outliers Removed)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Total Essay Character Length')\n",
    "plt.savefig('Essay_Length_vs_Age.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations \n",
    "\n",
    "* **No Clear Trend:** The points form a large, dense cloud rather than a distinct line. This confirms the very weak correlation (0.08) we saw in the heatmap. We can't predict a user's age based on how much they write.\n",
    "* **Highest Density in the 20-40 Age Range:** The densest area of the plot is for users between 20 and 40 years old with essay lengths under 5,000 characters. This is expected, as this is the largest user demographic.\n",
    "* **Most Users Write Moderately:** Regardless of age, the vast majority of users write less than 5,000 characters. Very long essays are rare across all age groups.\n",
    "\n",
    "This Chart shows that **`essay_length` by itself is a weak feature** for predicting age.\n",
    "\n",
    "To get predictive power from the essays, we will need to move beyond simple metrics like length and analyze the **actual content** of the text. The vocabulary, topics, and writing style are much more likely to change with age than the sheer volume of text. This reinfoces the possiblity of using more advanced NLP techniques (like TF-IDF or word embeddings) in the next stage of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to classify offspring status into simpler categories\n",
    "def clean_offspring(status):\n",
    "    if 'has kids' in status or 'has a kid' in status:\n",
    "        return 'has kids'\n",
    "    elif \"doesn't have kids\" in status or 'wants kids' in status:\n",
    "        return \"doesn't have kids\"\n",
    "    else:\n",
    "        return 'not specified'\n",
    "\n",
    "# Apply the function to create the new cleaned column\n",
    "profiles_filtered['offspring_cleaned'] = profiles_filtered['offspring'].apply(clean_offspring)\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a box plot to see age distribution by offspring status\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=profiles_filtered, x='offspring_cleaned', y='age')\n",
    "plt.title('Age Distribution by Offspring Status')\n",
    "plt.xlabel('Offspring Status')\n",
    "plt.ylabel('Age')\n",
    "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.savefig('Age_Distribution_by_Offspring_Status.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "* **Two Distinct Groups:** The visualization clearly separates the user base into two demographic groups: users who have kids and users who do not.\n",
    "* **Strong Separation:** There's a dramatic difference in age between these groups. The median age for users with kids (mid-40s) is significantly higher than for those without kids (late 20s). The boxes for these groups have almost no overlap.\n",
    "* **\"Not Specified\" Insight:** The \"not specified\" category has an age distribution nearly identical to the \"doesn't have kids\" category. This confirms that users who don't fill out this field are overwhelmingly in the younger, non-parent demographic.\n",
    "\n",
    "This confirms that `offspring_cleaned` column is an extremely strong and reliable feature for predicting `age`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This was the most critical phase of the project, where raw data was transformed into numerical features suitable for a machine learning model. A multi-step approach was used to handle the variety of data types present in the dataset.\n",
    "\n",
    "* **Text Simplification:** The 10 separate `essay` columns were combined into a single text block for each user. From this, simple metrics like total character count (`essay_length`) and word count (`word_count`) were calculated.\n",
    "* **Datetime and Location Features:** The `last_online` string was converted into a proper datetime object to extract numerical features like the year, month, and day of the week. Similarly, the `location` string was parsed to extract the user's `state`, creating a useful categorical feature.\n",
    "* **Categorical Simplification:** To reduce noise, messy categorical columns like `offspring`, `religion`, and `sign` were cleaned by custom functions to extract the core category (e.g., `\"gemini\"` from `\"gemini and it doesn't matter\"`).\n",
    "* **Multi-Label Feature Creation:** For columns like `ethnicity` and `speaks` where users could list multiple values (e.g., `'asian, white'`), a specialized `get_dummies` approach was used to create binary columns for each possible entry. This correctly captures users belonging to multiple groups.\n",
    "* **Final Encoding:** After all the cleaning and creation steps, the final set of categorical features was one-hot encoded. Simultaneously, **TF-IDF (Term Frequency-Inverse Document Frequency)** was applied to the combined essays to convert the text into a 500-column numerical matrix based on word importance.\n",
    "* **Assembly:** All engineered features—simple metrics, one-hot encoded columns, and the TF-IDF matrix—were combined into a final feature set containing **1002 features** ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Load and Initial Cleaning ---\n",
    "profiles = pd.read_csv('profiles.csv')\n",
    "profiles_filtered = profiles[profiles['age'] <= 65].copy()\n",
    "\n",
    "# --- 2. Impute All Missing Data ---\n",
    "essay_cols = [f\"essay{i}\" for i in range(10)]\n",
    "fill_values = {\n",
    "    'body_type': 'not specified', 'diet': 'not specified', 'drinks': 'not specified',\n",
    "    'drugs': 'not specified', 'education': 'not specified', 'ethnicity': 'not specified',\n",
    "    'job': 'not specified', 'offspring': 'not specified', 'pets': 'not specified',\n",
    "    'religion': 'not specified', 'sign': 'not specified', 'smokes': 'not specified',\n",
    "    'speaks': 'not specified', 'height': profiles_filtered['height'].median()\n",
    "}\n",
    "for col in essay_cols:\n",
    "    fill_values[col] = ''\n",
    "profiles_filtered.fillna(value=fill_values, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "\n",
    "# A) Combine text and engineer simple features\n",
    "profiles_filtered['all_essays'] = profiles_filtered[essay_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "profiles_filtered['essay_length'] = profiles_filtered['all_essays'].str.len()\n",
    "profiles_filtered['word_count'] = profiles_filtered['all_essays'].str.split().str.len()\n",
    "\n",
    "# B) Engineer location, datetime, and cleaned categorical features\n",
    "profiles_filtered[['city', 'state']] = profiles_filtered['location'].str.split(',', n=1, expand=True)\n",
    "profiles_filtered['state'] = profiles_filtered['state'].str.strip()\n",
    "profiles_filtered['last_online_dt'] = pd.to_datetime(profiles_filtered['last_online'], format='%Y-%m-%d-%H-%M')\n",
    "profiles_filtered['last_online_year'] = profiles_filtered['last_online_dt'].dt.year\n",
    "profiles_filtered['last_online_month'] = profiles_filtered['last_online_dt'].dt.month\n",
    "profiles_filtered['last_online_dayofweek'] = profiles_filtered['last_online_dt'].dt.dayofweek\n",
    "\n",
    "def clean_offspring(status):\n",
    "    if 'has kids' in status or 'has a kid' in status: return 'has kids'\n",
    "    elif \"doesn't have kids\" in status or 'wants kids' in status: return \"doesn't have kids\"\n",
    "    else: return 'not specified'\n",
    "profiles_filtered['offspring_cleaned'] = profiles_filtered['offspring'].apply(clean_offspring)\n",
    "\n",
    "def clean_religion(religion_str): return religion_str.split(\" \")[0]\n",
    "profiles_filtered['religion_cleaned'] = profiles_filtered['religion'].apply(clean_religion)\n",
    "\n",
    "def clean_sign(sign_str): return sign_str.split(\" \")[0]\n",
    "profiles_filtered['sign_cleaned'] = profiles_filtered['sign'].apply(clean_sign)\n",
    "\n",
    "# C) Create dummy columns for multi-label text features (ethnicity, speaks)\n",
    "ethnicity_dummies = profiles_filtered['ethnicity'].str.get_dummies(sep=', ')\n",
    "speaks_dummies = profiles_filtered['speaks'].str.get_dummies(sep=', ') # Added for 'speaks'\n",
    "profiles_with_dummies = pd.concat([profiles_filtered, ethnicity_dummies, speaks_dummies], axis=1)\n",
    "\n",
    "# D) One-Hot Encode the remaining categorical features\n",
    "features_to_encode = [\n",
    "    'body_type', 'diet', 'drinks', 'drugs', 'education', 'job', 'orientation',\n",
    "    'pets', 'smokes', 'status', 'sex', 'state',\n",
    "    'offspring_cleaned', 'religion_cleaned', 'sign_cleaned'\n",
    "]\n",
    "profiles_encoded = pd.get_dummies(profiles_with_dummies, columns=features_to_encode, drop_first=True)\n",
    "\n",
    "# E) Create advanced text features with TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(profiles_encoded['all_essays'])\n",
    "\n",
    "\n",
    "# --- 4. Assemble Final Data and Split for Modeling ---\n",
    "\n",
    "# A) Define the target variable\n",
    "y = profiles_encoded['age']\n",
    "\n",
    "# B) Select all final feature columns\n",
    "# --- UPDATED: Added 'speaks' to the drop list ---\n",
    "cols_to_drop_for_X = [\n",
    "    'age', 'all_essays', 'offspring', 'religion', 'sign', 'ethnicity', 'speaks', # Added 'speaks'\n",
    "    'location', 'city', 'last_online', 'last_online_dt'\n",
    "] + essay_cols\n",
    "numeric_and_ohe_features = profiles_encoded.drop(columns=cols_to_drop_for_X)\n",
    "\n",
    "# C) Combine all features into the final matrix X\n",
    "X = hstack([numeric_and_ohe_features.astype(float).values, tfidf_features])\n",
    "\n",
    "# D) Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# --- 5. Verification ---\n",
    "print(\"Feature engineering and data preparation complete.\")\n",
    "print(\"Final training feature matrix shape:\", X_train.shape)\n",
    "print(\"Final testing feature matrix shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Baseline Moded (Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize the Model\n",
    "# We use a standard alpha (regularization strength).\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# 2. Train the Model\n",
    "# The model learns the relationships between features and the age from the training data.\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 3. Make Predictions on the Test Data\n",
    "# The model now predicts the age for the users in the test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 4. Evaluate the Model's Performance\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Model Performance\n",
    "\n",
    "This is a **solid baseline result**. The model can explain about half of the variation in user age, and its predictions are, on average, off by just under 5 years.\n",
    "\n",
    "### Performance Breakdown\n",
    "\n",
    "* **R-squared (R²): 0.52:** Features explain **52% of the variance** in user ages. This indicates that the features engineered—especially from `education`, `offspring`, and the essays—have significant predictive power.\n",
    "* **Mean Absolute Error (MAE): 4.84 years:** On average, our model's prediction of a user's age is off by about **4.84 years**. Given the age range of 18-65, this is a reasonable starting point.\n",
    "\n",
    "---\n",
    "### How to Improve the Model\n",
    "\n",
    "1.  **More Powerful Models:** Tree-based models are excellent for this type of problem. The next logical step would be to train a **`RandomForestRegressor`** or a **`GradientBoostingRegressor`** from Scikit-learn. These models can often uncover more complex patterns than a linear model like Ridge.\n",
    "2.  **Hyperparameter Tuning:** You can fine-tune your model's settings (hyperparameters) using `GridSearchCV` or `RandomizedSearchCV` to squeeze out better performance.\n",
    "3.  **Advanced Feature Engineering:** We could experiment with more advanced NLP techniques like **word embeddings** (e.g., Word2Vec) instead of TF-IDF to better capture the meaning of the essay text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize the Random Forest Model\n",
    "# n_estimators is the number of trees in the forest.\n",
    "# n_jobs=-1 uses all available CPU cores to speed up training.\n",
    "# random_state=42 ensures the results are reproducible.\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 2. Train the Model\n",
    "# This will take longer than the Ridge model.\n",
    "print(\"Training the Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 3. Make Predictions on the Test Data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 4. Evaluate the Model's Performance\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\n--- Random Forest Model Evaluation ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_rf:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_rf:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a grid of parameters to search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Randomized Search\n",
    "# It will try 10 different combinations from the grid above\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=10, \n",
    "    cv=3, \n",
    "    verbose=2, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search to find the best model\n",
    "print(\"Performing randomized search for best hyperparameters...\")\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "\n",
    "# You can then use rf_random_search.best_estimator_ as your new, tuned model\n",
    "print(f\"Best parameters found: {rf_random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the trained model\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = numeric_and_ohe_features.columns.tolist() + tfidf_vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "# Create a DataFrame for easy sorting and plotting\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot the top 20 most important features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20))\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.savefig('Distribution_of_User_Ages.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Analysis and Conclusion\n",
    "\n",
    "### Feature Importance Insights\n",
    "\n",
    "The feature importance plot is a fantastic validation the EDA.\n",
    "\n",
    "* **Top Predictors Confirmed:** The model found that **`offspring_cleaned_has kids`** is by far the most important feature for predicting age. This, along with the high importance of education-related features (`education_working on college/university`, `job_student`, etc.), confirms that **key life-stage indicators are the strongest predictors**.\n",
    "* **Logical Model:** This result shows that our model is learning logical, real-world patterns. It's not relying on obscure signals but on the features we hypothesized would be most impactful.\n",
    "* **Text Features:** While many individual text features from the essays appear in the top 20, they have much lower importance scores than the top categorical features. This suggests that while text is helpful, the demographic and life-stage data provides the core predictive power.\n",
    "\n",
    "### Hyperparameter Tuning and Final Model Choice\n",
    "\n",
    "The randomized search for the Random Forest model settled on parameters that are very close to the default settings (`'max_depth': None`, etc.). This means that even after searching, the complex Random Forest model couldn't find a configuration that significantly outperformed its baseline, which was already weaker than the Ridge model.\n",
    "\n",
    "The \"winning\" model is the **Ridge Regression model**.\n",
    "\n",
    "* **Final R-squared (R²): 0.52**\n",
    "* **Final Mean Absolute Error (MAE): 4.84 years**\n",
    "\n",
    "### Project Summary \n",
    "\n",
    "We have prepared a complex, real-world dataset, engineered dozens of features from categorical and text data, and built and evaluated multiple models.\n",
    "\n",
    "The final result is a model that can predict a user's age with an average error of **~4.8 years**, explaining about 52% of the variance in the data. The analysis concludes that for this specific dataset, a simpler, regularized linear model (Ridge) was more effective than a more complex, tree-based model (Random Forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps \n",
    "\n",
    "Fine-tuning the model further will likely yield only minor improvements and is far less valuable. Therfore we will create a simple web application using a tool like Streamlit or Flask. The app could have a simple form where a user enters some hypothetical data (e.g., selects their education, offspring status, and writes a short \"about me\" summary), and the model predicts their age."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
